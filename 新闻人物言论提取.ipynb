{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "新闻人物言论提取.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaobingbing/AI-For-NLP/blob/master/%E6%96%B0%E9%97%BB%E4%BA%BA%E7%89%A9%E8%A8%80%E8%AE%BA%E6%8F%90%E5%8F%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E9YQ5h1L5fb",
        "colab_type": "code",
        "outputId": "36ea21f6-067a-4475-9c81-1f6d6e1f9a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzdbBGgvbAGH",
        "colab_type": "code",
        "outputId": "6b256f9f-8513-4a14-cccd-87afc85eb123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install pyltp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyltp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/72/2d88c54618cf4d8916832950374a6f265e12289fa9870aeb340800a28a62/pyltp-0.2.1.tar.gz (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 4.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyltp\n",
            "  Building wheel for pyltp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyltp: filename=pyltp-0.2.1-cp36-cp36m-linux_x86_64.whl size=32058622 sha256=98cd2383f6eeffaa844bf20d8f7248a3146ac2f6fdce7d224d937ff0a6e9d79b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/3a/35/b11293efb2c77c0e7b6fa574271d51cddd9abd1f634535343c\n",
            "Successfully built pyltp\n",
            "Installing collected packages: pyltp\n",
            "Successfully installed pyltp-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KhA2-aeV45L",
        "colab_type": "text"
      },
      "source": [
        "### 对句子成分进行解析\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6El1BUISP4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyltp\n",
        "from gensim.models import Word2Vec\n",
        "import jieba\n",
        "import re\n",
        "from gensim.models.word2vec import LineSentence\n",
        "from pyltp import SentenceSplitter,NamedEntityRecognizer,Postagger,Parser,Segmentor\n",
        "from gensim import models\n",
        "import numpy as np\n",
        "\n",
        "model_path = './drive/My Drive/model/ltp_data_v3.4.0/'\n",
        "\n",
        "cws_model = model_path + \"cws.model\"\n",
        "pos_model = model_path + \"pos.model\"\n",
        "par_model = model_path + \"parser.model\"\n",
        "ner_model = model_path + \"ner.model\"\n",
        "\n",
        "def get_word_list(sentence, model):\n",
        "    # 得到分词\n",
        "    segmentor = Segmentor()\n",
        "    segmentor.load(model)\n",
        "    word_list = list(segmentor.segment(sentence))\n",
        "    segmentor.release()\n",
        "    return word_list\n",
        "\n",
        "def get_postag_list(word_list, model):\n",
        "    # 得到词性标注\n",
        "    postag = Postagger()\n",
        "    postag.load(model)\n",
        "    postag_list = list(postag.postag(word_list))\n",
        "    postag.release()\n",
        "    return postag_list\n",
        "\n",
        "def get_parser_list(word_list,postag_list,model):\n",
        "    #得到依存关系\n",
        "    parser = Parser()\n",
        "    parser.load(model)\n",
        "    arcs = parser.parse(word_list,postag_list)\n",
        "    arc_list = [(arc.head,arc.relation) for arc in arcs]\n",
        "    parser.release()\n",
        "    return arc_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBnGma1OXz0z",
        "colab_type": "text"
      },
      "source": [
        "### 加载词向量，寻找类似单词"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HYcsUskhEBC",
        "colab_type": "code",
        "outputId": "7dc18f57-06af-4acf-9a30-199792e17ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "word2vec = Word2Vec.load('./drive/My Drive/model/news_word2vec.model')\n",
        "\n",
        "def get_similar_word_list(word,top=10): \n",
        "    #取得与word最相近的10个词\n",
        "    word_list = []\n",
        "    for w in word2vec.wv.most_similar(word,topn=top):\n",
        "        word_list.append(w[0])\n",
        "    return word_list\n",
        "\n",
        "def gather_similar_word(word): \n",
        "    #广度优先搜索 取得深度为10 与word 相近的所有词 \n",
        "    words = [word]\n",
        "    seen = set()\n",
        "    depth = 10\n",
        "    while words and depth > 0:\n",
        "        word = words.pop(0)\n",
        "        for w in word:\n",
        "            if w not in seen:\n",
        "                seen.add(w)\n",
        "                words.append(get_similar_word_list(w,10))\n",
        "            else:\n",
        "                continue\n",
        "        depth -= 1\n",
        "    return list(seen)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48oGpJMMjdZ7",
        "colab_type": "code",
        "outputId": "4c0e34d5-12e1-41c2-9971-cf8de3bc8227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "spoken_words = gather_similar_word(\"说\")\n",
        "spoken_words.append(\"报道\") \n",
        "print(spoken_words)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['说法', '特别强调', '坦言', '透露', '告诉', '称', '问', '所说', '了解', '看来', '说', '见到', '相信', '建议', '眼中', '明说', '中说', '来说', '获悉', '介绍', '文说', '地说', '看到', '普遍认为', '指出', '认为', '强调', '表示', '还称', '中称', '直言', '报道']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ5HAXemjohI",
        "colab_type": "text"
      },
      "source": [
        "### get SBV relations in corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYOypN5LjjZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_spoken_word_id_and_sub(spoken_words, word_list, parser_list):\n",
        "    id_list = []\n",
        "    \n",
        "    for sub_id, parse_relation in enumerate(parser_list):\n",
        "        index, relation = parse_relation\n",
        "        if relation == 'SBV':\n",
        "            spoken_word = word_list[index - 1]\n",
        "            if spoken_word in spoken_words:\n",
        "                word_id = index - 1\n",
        "                id_list.append((sub_id, word_id))\n",
        "    \n",
        "    # 返回值对应主语和说所在的位置\n",
        "    return id_list\n",
        "\n",
        "\n",
        "def get_sentence(news, word_list, idx):\n",
        "    # 取得说的内容 和 SBV 的宾语成分\n",
        "    # idx 为表示说的次在新闻中的位置信息\n",
        "    \n",
        "    # 说后面第一个开头的汉字对应位置\n",
        "    index = len(\"\".join(word_list[:idx + 1]))\n",
        "    \n",
        "    # 寻找第一个句子\n",
        "    stop1 = news[index + 1:].find(\"。\")\n",
        "    stop2 = news[index + 1:].find(\"!\")\n",
        "    stop3 = news[index + 1:].find(\"?\")\n",
        "    \n",
        "    # 检查后面是否还有句子\n",
        "    stop_list = [stop for stop in [stop1, stop2, stop3] if stop != -1]\n",
        "    if stop_list == None:\n",
        "        False\n",
        "        \n",
        "    # 返回后面的第一个句子\n",
        "    stop = min(stop_list)\n",
        "    \n",
        "    # 检查后面的句子是否有双引号引起来的句子\n",
        "    begin = 9999\n",
        "    end = 9999\n",
        "    if \"“\" in news[index + 1:] and \"”\" in news[index + 1:]:\n",
        "        begin = news[index + 1:].find(\"“\")\n",
        "        end = news[index + 1:].find(\"”\")\n",
        "        \n",
        "    # 第一种情况,双引号在stop前面,表明说词后面的双引号的句子为要说的内容\n",
        "    if begin < stop:\n",
        "        result = news[index + 1 + begin: index + 1 + end + 1]\n",
        "        \n",
        "    # 第二种情况，双引号的内容在第一个句子后或者无双引号，说词后的句子即为说的内容\n",
        "    else:\n",
        "        sentence = news[index + 2: stop + index + 2].strip()\n",
        "        result = sentence\n",
        "        next_id = stop + index + 2\n",
        "        sim = 1\n",
        "        # 现在要对该情况进行判断，判断下一个句子和该句之间的关联性，通过检查句子之间的相似度之间的关系\n",
        "        # 如果相似度大于0.7表明相似(可以自行设定)\n",
        "        while sim > 0.75 and  next_id < len(news) - 1:\n",
        "            next_sentence_id = next_id\n",
        "            if next_sentence_id < len(news) - 1:\n",
        "                next_sentence, next_id = get_next_sentence(news, next_sentence_id)\n",
        "                sim = sentence_distance(sentence, next_sentence)\n",
        "                \n",
        "            if sim > 0.75:\n",
        "                result += next_sentence\n",
        "                sentence = next_sentence\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def get_next_sentence(news, index):\n",
        "    # 取得index后面的第一句话\n",
        "    stop1 = news[index + 1:].find(\"。\")\n",
        "    stop2 = news[index + 1:].find(\"!\")\n",
        "    stop3 = news[index + 1:].find(\"?\")\n",
        "    \n",
        "    stop_list = [stop for stop in [stop1,stop2,stop3] if stop != -1]\n",
        "    if stop_list is None:\n",
        "        False\n",
        "    stop = min(stop_list)\n",
        "    return news[index:index+stop+2],index+stop+2\n",
        "\n",
        "def cut(sentence):\n",
        "    return \" \".join(jieba.cut(sentence))\n",
        "\n",
        "def sentence_distance(sentence1, sentence2):\n",
        "    word_list_1 = cut(sentence1).split()\n",
        "    word_list_2 = cut(sentence2).split()\n",
        "    \n",
        "    vec_1 = 0\n",
        "    vec_2 = 0\n",
        "    \n",
        "    for i in range(len(word_list_1)):\n",
        "        if word_list_1[i] in word2vec.wv.vocab:\n",
        "            vec_1 += word2vec.wv[word_list_1[i]]\n",
        "    \n",
        "    for i in range(len(word_list_2)):\n",
        "        if word_list_2[i] in word2vec.wv.vocab:\n",
        "            vec_2 += word2vec.wv[word_list_2[i]]\n",
        "            \n",
        "    return np.dot(vec_1, vec_2)/(np.linalg.norm(vec_1)*np.linalg.norm(vec_2))\n",
        "\n",
        "def get_sub_and_view(idxs, news, word_list):\n",
        "    # 取得主语以及要说的内容\n",
        "    sub = []\n",
        "    speech = []\n",
        "    for sub_id, spoken_id in idxs:\n",
        "        sub.append(word_list[sub_id])\n",
        "        speech.append(get_sentence(news, word_list, spoken_id))\n",
        "        \n",
        "    return sub,speech"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVNrXbpDf7gZ",
        "colab_type": "text"
      },
      "source": [
        "### 测试可行性"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvA7RVp8fKxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"英国央行行长卡尼周二指出，目前不是加息良机，加息将是受限且渐进的，未来几个月将关注消费增长疲弱被其他需求因素抵消的程度，以及经济如何对脱欧谈判做出反应。美联储副主席Fischer(永久投票权)称，美国金融系统比危机之前要强，不良贷款减少。长期低利率可能已经抬高了住房价格。需要进一步努力加强住房金融系统。宏观审慎工具可以缓解住房市场危机。不准备对货币政策和美国经济前景置评。美国芝加哥联储主席Evans(有投票权)称，可能将下一次加息推迟到12月份，以留出时间评估数据。看到多个月份的通胀数据改善至关重要。根据近期数据，通胀难以达到2%。希望见到通胀上行，而不只是平稳。\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLZ1o-AZfEbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b1e48277-3fc4-4e77-d3f2-627ccc3cd2df"
      },
      "source": [
        "word_list = get_word_list(text,cws_model)\n",
        "postag_list = get_postag_list(word_list,pos_model)\n",
        "parser_list = get_parser_list(word_list,postag_list,par_model)\n",
        "\n",
        "idx=find_spoken_word_id_and_sub(spoken_words,word_list,parser_list)\n",
        "idx\n",
        "\n",
        "sub,speech = get_sub_and_view(idx,text,word_list)\n",
        "\n",
        "for i in range(len(sub)):\n",
        "    print(sub[i],speech[i])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "卡尼周二 前不是加息良机，加息将是受限且渐进的，未来几个月将关注消费增长疲弱被其他需求因素抵消的程度，以及经济如何对脱欧谈判做出反应。\n",
            "Fischer 国金融系统比危机之前要强，不良贷款减少。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3cjXBe0hISF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "77f30dc2-f770-430e-f129-e0ec17055374"
      },
      "source": [
        "for i in range(len(sub)):\n",
        "    print(sub[i],speech[i])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "卡尼周二 前不是加息良机，加息将是受限且渐进的，未来几个月将关注消费增长疲弱被其他需求因素抵消的程度，以及经济如何对脱欧谈判做出反应。\n",
            "Fischer 国金融系统比危机之前要强，不良贷款减少。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iza9Psh0hOWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}